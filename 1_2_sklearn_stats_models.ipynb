{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciKit learn: Statistical learning\n",
    "\n",
    "Based on materials from: http://scikit-learn.org/stable/tutorial/statistical_inference/settings.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical learning: The setting and the estimator object in scikit-learn\n",
    "\n",
    "* Data dealed with as 2D arrays\n",
    "* We can reshape them, by for instance transforming 8x8 images into 64-element feature vectors\n",
    "* *Estimator* is any object learning from data (classification, regression or clustering algorithm, or *transformer* extracting useful features from raw data)\n",
    "* We can access estimated parameters from an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General syntax for estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator = Estimator(param1=1, param2=2)\n",
    "#estimator.param1\n",
    "#estimator.estimated_param_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning: Predicting an output variable from high-dimensional observations\n",
    "\n",
    "We try learning link between two datasets: Observed data x and external variable y that we are trying to predict - usually called 'targets' or 'labels'.\n",
    "\n",
    "### Nearest neighbor and the curse of dimensionality\n",
    "\n",
    "To be effective we need distance between neighboring points to be less than some value d, for which the size depends on the problem. In one dimension this requires `n ~ 1/d` points. If we scale this up to `P` features we now require `n ~ 1/d^p` points. The number of training points for a good estimator grows exponentially.\n",
    "\n",
    "This is called *the curse of dimensionality* and is core problem addressed by machine learning.\n",
    "\n",
    "### Linear model: From regression to sparsity\n",
    "\n",
    "* In its simplest form, we try minimizing sum of squared residuals of the model.\n",
    "* If few datapoints per dimension noise in observations results in high variance.\n",
    "* One solution in high-dimensional statistical learning is to *shrink* regression coefficients to zero - any randomly chosen set of observations are likely to be uncorrelated - This is called *Ridge regression*\n",
    "    * There is a `bias <-> variance` tradeoff for the `alpha` parameter\n",
    "* Aha - visualizing in 10 dimensions. Hard to think about, but would be fairly 'empty space'\n",
    "* One way to mitigate curse of dimensionality is to select only informative features\n",
    "    * Ridge regression is one way to decrese contribution (but doesn't set to zero)\n",
    "    * Another method is Lasso (least absolute shrinkage and selection operator) which can set some coefficients to zero - This can be seen as a sparse method\n",
    "    * LassoLars is scikit-learn implementation able to take on this\n",
    "\n",
    "### Classification\n",
    "\n",
    "* For classification linear regression is not right approach as it gives too much weight to data far from decision frontier\n",
    "    * A linear approach is to fit *sigmoid function* or *logistic function*\n",
    "    \n",
    "### Support Vector Machine\n",
    "\n",
    "* Tries maximizing margin between to classes\n",
    "* Adjusting regularization parameter C decides how many of observations that are involved in separation\n",
    "* Can also use *kernel tricky* using other boundary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection: Choosing estimators and their parameters\n",
    "\n",
    "* Every estimator exposes *score* method that can judge quality of fit (or prediction)\n",
    "* To better access prediction accuracy we can split data into *folds* used for training and testing\n",
    "\n",
    "## Cross-validation generators\n",
    "\n",
    "Using split method data can easily be split into subsets. This facilitates ease of cross-validation. Then the cross-validation score can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2 3 4 5] | test: [0 1]\n",
      "Train: [0 1 4 5] | test: [2 3]\n",
      "Train: [0 1 2 3] | test: [4 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "X = ['a', 'a', 'b', 'c', 'c', 'c']\n",
    "k_fold = KFold(n_splits=3)\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    print('Train: {} | test: {}'.format(train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])\n",
    "#     for train, test in k_fold.split(X_digits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different cross-validation generators\n",
    "\n",
    "* KFold: Splits data into K folds, trains on K-1 and tests on left-out\n",
    "* StratifiedKFold: Preseves class distribution within each  fold\n",
    "* GroupKFold: Ensures same group is not in both testing and training sets\n",
    "* ShuffleSplit: Genererates train/test indices based on random permutation\n",
    "* StratifiedShuffleSplit: Preserves class distribution within each distribution\n",
    "* LeaveOneGroupOut: Takes a group array to group observations (?)\n",
    "* LeavePGroupsOut: Leave P groups out\n",
    "* LeaveOneOut: Leave one observation out\n",
    "* LeavePOut: Leave P observations out\n",
    "* PredefinedSplit: Generates train/test indices based on predefined splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: Grouping observations together\n",
    "\n",
    "We can try splitting a dataset into defined separate groups.\n",
    "\n",
    "## Hierarchical agglomerative clustering\n",
    "\n",
    "Each observation starts in cluster, and then iteratively merged in a way minimizing *linkage criterion*. In particular efficient when clusters consist of few observations. For large number of clusters: Much more efficient than k-means.\n",
    "\n",
    "Divisive: Topdown approaches starting in one cluster, and then iteratively splitting down the hierarchy. Slow and not strong for many clusters.\n",
    "\n",
    "## Decompositions: From a signal to components and loadings\n",
    "\n",
    "PCA: Selects successive components explaining maximum variance in the signal. We can use this to transform data to reduce dimensionality by projecting on principal subspace.\n",
    "\n",
    "Independent component analysis (ICA) select components so that distribution of loadings carries maximum independent information. It is able to recover non-Gaussian independent signals. (Interesting! What happens if applied to omics-dataset? What would it mean - what information would be recovered?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
